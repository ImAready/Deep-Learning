## 신경망 

가중치 매개변수(w1, w2)의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 신경망의 중요한 성질이다.

신경망의 개요와 신경망이 입력을 식별하는 처리 과정을 정리한다. 데이터에서 가중치 매개변수 값을 학습하는 방법은 다음장에서 알아본다.



#### 3.1 퍼셉트론 -> 신경망



- 그림 신경망

![1527641515953](C:\Users\wips\AppData\Local\Temp\1527641515953.png)

은닉층은 입력,출력층과 달리 사람 눈에는 보이지 않는다. 

x1, x2 2개의 입력 신호를 받아 y를 출력하는 퍼셉트론을 수식으로 나타내면

![1527643055959](C:\Users\wips\AppData\Local\Temp\1527643055959.png)

여기서 편향 b는 뉴런이 얼마나 쉽게 활성화되느냐를 제어하고, 가중치 w1, w2는 각 신호의 영향력을 제어한다. 퍼셉트론 구조에 편향을 명시한다면 아래와 같이 나타낼 수 있다.

![1527643158357](C:\Users\wips\AppData\Local\Temp\1527643158357.png)

위 그림에서 가중치가 b이고 입력이 1인 뉴런이 추가되었다. 이 퍼셉트론은 x1, x2, 1 3개의 입력신호가 뉴런에 입력되어 각 신호에 가중치 b, w1, w2를 곱한 후 다음 뉴런에 전달된다. 이 다음 뉴런에서는 입력받은 신호의 값을 더하여 그 합이 0을 넘으면 1을 출력하고 그렇지 않으면 0을 출력한다. 

 ###### 편향의 입력 신호는 항상 1이기 때문에 회색으로 다른 뉴런과 구분을 둠



- 위 퍼셉트론 수식을 간결한 형태로 작성하면

![1527643444058](C:\Users\wips\AppData\Local\Temp\1527643444058.png)

0을 넘으면 1을 출력하고 그렇지 않으면 0을 출력하는 동작을 하나의 함수로 나타내고 이 함수를 h(x)라고 표현한 것이다.

입력 신호의 총 합이 위 h(x) 함수를 거쳐 변환된 값이 y의 출력이 된다. 이처럼 입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 **활성화 함수**라고 한다. 

###### 입력 신호의 총합이 활성화를 일으키는지 정하는 역할(0보다 큰지 아닌지)######

여기서 가중치가 달린 입력 신호와 편향의 총합 계산 식을 a라고 하고 함수 h()에 넣어 y를 출력하는 흐름은 아래와 같다.

![1527664958004](C:\Users\wips\AppData\Local\Temp\1527664958004.png)

기존 y 뉴런의 원을 키우고 그 안에 활성화 함수의 처리 과정을 명시적으로 그려 넣은 것. 즉, 가중치 신호를 조합한 결과가 a노드이고 활성화 함수 h()를 통과하여 y라는 노드로 변환되는 과정을 명시한 것.  

###### 노드 = 뉴런######



#### 3.2 활성화 함수####

위 (0보다 크면 1) 활성화 함수는 임계값을 경계로 출력이 바뀌는데, 이런 함수를 계단 함수라고 한다.  활성화 함수로 여러 함수를 사용 할 수 있는데 퍼셉트론은 그 중에서 계단 함수를 사용한다. 



##### 3.2.1 퍼셉트론에서 사용하는 계단 함수 #####

![1527667177562](C:\Users\wips\AppData\Local\Temp\1527667177562.png)

계단 함수는 입력 값 x가 0보다 크면 1 작으면 0을 출력하는 함수이다. 

첫번째 계단 함수의 경우 인수 x는 실수(부동소수점)만 받아들이기 때문에 입력 x로 넘파이 배열을 넣을 수는 없다. 즉, step_funtion(np.array([1.0 ,2.0]))이 불가능 하다는 말이다. 따라서 두번째 함수처럼 넘파이 배열도 지원하기 위한 수정 함수이다.

- 두 번째 함수를 한 줄씩(인터프리터 사용) 실행해 보면 아래와 같음.

![1527667456614](C:\Users\wips\AppData\Local\Temp\1527667456614.png)

넘파이 배열에 부등호(=) 연산을 수행하면 배열의 각 원소에 부등호 연산을 수행한 bool 배열이 반환된다. 

###### (각각의 배열 원소 x를 0보다 큰지 연산하여 0보다 크면 true(1), 0 이하면 false(0)로 변환한 배열)######

이 bool 배열을 0이나 1의 int 형으로 출력하기 위해 두번째 함수 마지막에 y.astype(np.int)를 해준다. 

###### astpye() : 넘파이 배열의 자료형을 변환할 때 사용######



- 이 계단 함수를 그래프로 그려보면

![1527668005063](C:\Users\wips\AppData\Local\Temp\1527668005063.png)



...왜 계단이 기울었어??

위 처럼 그래프가 계단 형태를 띄기 때문에 계단 함수하고 부름.



##### 3.2.2 신경망에서 사용하는 시그모이드 함수

![1527666203144](C:\Users\wips\AppData\Local\Temp\1527666203144.png)

*e = 2.7182...*

함수는 입력을 주면 출력을 반환하는 변환기로, 신경망은 위 함수를 이용하여 신호를 변환하고, 그 변환된 신호를 다음 뉴런에 전달한다. 



- 위 수식을 파이썬 인터프리터로 한 줄씩 실행해보면

![1527668596722](C:\Users\wips\AppData\Local\Temp\1527668596722.png)

브로드캐스트 기능 덕분에 잘 처리한다.

*브로드캐스트 : 넘파이 배열과 스칼라값의 연산을 넘파이 배열의 원소 각각과 스칼라 값의연산으로 바꿔 수행, 즉 배열 각각의 원소에 연산*..?



- 그래프를 그려보면

![1527669247003](C:\Users\wips\AppData\Local\Temp\1527669247003.png)

??????????????..??????????????????????

시그모이드는..S자 모양이라는 뜻이다.



##### 3.2.3 시그모이드 vs 계단 함수#####

![1527669301053](C:\Users\wips\AppData\Local\Temp\1527669301053.png)

두 함수를 함께 그려보면 위와 같음. 두 그래프의 **차이**는 "매끄러움"이다.

시그모이드는 부드러운 곡선으로 입력에 따라 출력이 연속적으로 변화한다. 하지만 계단 함수는 0을 경계로 값이 갑자기 바뀐다. 

시그모이드의 매끈함이 신경망 학습에서 아주 중요한 역할을 한다. 

시그모이드가 부드러운 곡선을 그리는 이유는 계단 함수는 출력값으로 0과 1만을 반환하지만 시그모이드는 (0.731...., 0.880....) 실수를 돌려주기 때문이다. 다시말해서 퍼셉트론에서는 뉴런(노드) 사이에 0혹은 1이 흘렀다면 신경망에서는 연속적인 실수가 흐른다.

두 그래프의 **공통점**은 큰 관점에서 같은 모양을 하고 있는 것이다. 둘 다 입력값이 작을 때의 출력은 0에 가깝고, 입력이 커지면 1에 가까워지는 구조.



##### 3.2.4 비선형 함수#####

시그모이드와 계단함수의 공통점은 그 밖에도 있다. 둘 모두가 **비선형 함수**라는 것.

신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다. *층을 깊이 하는 의미가 없어지기 때문에*



##### 3.2.5 ReLu 함수#####

최근 시그모이드 함수 대신 ReLu함수를 이용한다. 

ReLu함수는 입력이 0을 넘으면 입력을 그대로 출력하고, 0 이하이면 0을 출력한다. 

수식으로 나타내면 아래와 같다.

![1527670303736](C:\Users\wips\AppData\Local\Temp\1527670303736.png)

- 구현을 해보면..

![1527670237362](C:\Users\wips\AppData\Local\Temp\1527670237362.png)

안된다..



- maximum함수 사용 => 두 입력 중 큰 값을 선택해 반환하는 함수.

![1527670344708](C:\Users\wips\AppData\Local\Temp\1527670344708.png)

된다...





#### 3.3 다차원 배열의 계산###

##### 3.3.1 다차원 배열#####

- numpy.array([1,2,3,4]) // 1차원 배열

- numpy.array([1,2], [3,4]) // 2차원 배열

| 함수              | 설명                                         |
| ----------------- | -------------------------------------------- |
| ndim(넘파이 배열) | 배열의 차원 수를 반환한다.                   |
| 넘파이 배열.shape | 배열의 형상을 튜플 형태로 반환한다. ex) 3X2, |

![1527744684285](C:\Users\wips\AppData\Local\Temp\1527744684285.png)

##### 3.2.2 행렬 곱#####

![1527744756317](C:\Users\wips\AppData\Local\Temp\1527744756317.png)

| 함수                                  | 설명                                              |
| ------------------------------------- | ------------------------------------------------- |
| numpy.dot(넘파이 배열1, 넘파이 배열2) | 넘파이 배열 2개를 인수로 받아 그 내적을 반환한다. |

넘파이 배열의 행렬 곱을 계산 할때는 실제와 같이 시행

ex) (3X**2**) X (**2**X3) = 가능

​      (3X**2**) X (**3**X2) = 불가능



##### 3.3.3 신경망의 내적#####

편향과 활성화 함수를 생략하고 가중치만 갖는 신경망을 가정한다.

![1527745052619](C:\Users\wips\AppData\Local\Temp\1527745052619.png)



#### 3.4 3층 신경망 구현####

3층 신경망에서 수행되는 **입력**부터 **출력**까지 처리를 구현한다.

아래 그림과 같이 은닉층 2개를 갖는 3층 신경망을 가정.

![1527745213232](C:\Users\wips\AppData\Local\Temp\1527745213232.png)

##### 3.4.1 표기법#####

![1527745268527](C:\Users\wips\AppData\Local\Temp\1527745268527.png)

오른쪽 위 (1) : 1층의 가중치, 1층의 뉴런임을 뜻한다.

가중치 오늘쪽 아래 : 인덱스 번호, (다음 층 번호, 앞층 번호)



##### 3.4.2 각 층의 신호 전달 구현#####

- 1층 

  ![1527745451482](C:\Users\wips\AppData\Local\Temp\1527745451482.png)

  

편향을 뜻하는 뉴런 1이 추가되었다. 

*편향은 오른쪽 아래 인덱스가 하나밖에 없다*  <?? *이는 앞 층의 편향 뉴런(편향1)이 하나뿐이기 때문이다*

출력 a1을 가중치와 2개의 입력 신호의 곱을 편향과 합한 수식으로 나타내면,

![1527745582346](C:\Users\wips\AppData\Local\Temp\1527745582346.png)

이 수식을 행렬의 내적을 이용하면 1층의 "가중치 부분"을 다음 식처럼 간소화 할 수 있다.

![1527745684095](C:\Users\wips\AppData\Local\Temp\1527745684095.png)

![1527745722859](C:\Users\wips\AppData\Local\Temp\1527745722859.png)

???

--------------------

파이썬으로 1층 구현

--------------------------------

1. 다음 층 출력 값 구하기

![1527746303223](C:\Users\wips\AppData\Local\Temp\1527746303223.png)

2. 활성화 함수 추가

![1527747152557](C:\Users\wips\AppData\Local\Temp\1527747152557.png)

은닉층에서의 가중치 합(가중 신호와 편향의 총합)을 a로 표기하고

###### w1 + w2 + b1  = a1???######

활성화 함수 h()로 변환된 신호를 z로 표기한다. 여기서 활선화 함수로 시그모이드 함수를 사용하기로 한다.

![1527747536937](C:\Users\wips\AppData\Local\Temp\1527747536937.png)

---------------------------

파이썬으로 2층 구현

-------------------------------------

![1527747894841](C:\Users\wips\AppData\Local\Temp\1527747894841.png)



-----------------------------

파이썬으로 마지막 층 전달 구현

---------------------------

![1527751651047](C:\Users\wips\AppData\Local\Temp\1527751651047.png)

마지막 처리의 경우 활성화 함수만 다르다.

여기서는 활성화 함수로 identity_function()을 정의했다. 이 함수는 항등 함수로 입력을 그대로 출력하는 함수이다. 때문에 굳이 정의할 필요는 없지만, 이전의 흐름과 동일하게 하기 위해서 구현했다.

###### 출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 정한다. 회귀에는 항등 함수, 2클래스 분류에는 시그모이드 함수, 다중 클래스 분류에는 소프트맥스 함수를 사용하는 것이 일반적이다######



##### 3.4.3 최종 구현#####

지금까지의 구현을 정리해서 신경망 구현의 관례에 따라 가중치만 W1과 같이 대문자로 쓰고, 그외 편향과 중간 결과등은 모두 소문자로 쓴다.

![1527753386903](C:\Users\wips\AppData\Local\Temp\1527753386903.png)





#### 3.5 출력층 설계####

신경망은 분류와 회귀 모두에 이용할 수 있다. 다만 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수가 달라진다. 일반적으로 회귀는 항등, 분류에는 소프트맥스 함수를 사용한다.



##### 3.5.1 항등 함수와 소프트맥스 함수 구현#####

- 항등 함수 : 입력을 그대로 출력한다.  입력과 출력이 항상 같다.

![1527754047784](C:\Users\wips\AppData\Local\Temp\1527754047784.png)

- 소프트맥스 함수 : 시그모이드를 정규화 한 것. 입력받은 값을 출력으로 0~1 사이의 값으로 정규화하며 출력 값들의 총합은 항상 1이 되는 함수로 확률계산에 사용.

![1527754215273](C:\Users\wips\AppData\Local\Temp\1527754215273.png)

​	n : 출력층의 뉴런 수

​	yk : 그중 k번째 출력

소프트맥스의 출력은 모든 입력 신호로부터 화살표를 받는다. 그림으로 나타내면 아래

 ![1527754398258](C:\Users\wips\AppData\Local\Temp\1527754398258.png)

- 인터프리터로 한 줄씩 보면

![1527754573045](C:\Users\wips\AppData\Local\Temp\1527754573045.png)

- 위 처리과정을 파이썬 함수로 구현

![1527754785440](C:\Users\wips\AppData\Local\Temp\1527754785440.png)



##### 3.5.2 소프트맥스 함수 구현시 주의점#####

**오버플로우** 문제를 주의해야 한다.

- 개선한 수식

![1527754865743](C:\Users\wips\AppData\Local\Temp\1527754865743.png)

음.......

1. C라는 임의의 정수를 분자와 분모 양쪽에 곱한다.
2. C를 지수함수 exp() 안으로 옮겨서 logC로 만든다
3. logC를 새로운 기호로 바꾼다.

위에서 말하는 것은 *소프트맥스의 지수 함수를 계산할 때 어떤 정수를 더해도 (혹은 빼도) 결과를 바뀌지 않는 다*는 것이다. 여기서 logC에 어떤 값을 대입해도 상관없지만, **오버플로우를 막을 목적**으로는 입력 신호 중 **최댓값**을 이용하는 것이 일반적이다.



- 위 식을 인터프리터로 한 줄씩 

![1527755210170](C:\Users\wips\AppData\Local\Temp\1527755210170.png)

아무 조치가 없다면 nan이 출력. 하지만 입력 신호 중 최댓값(c)를 빼주면 계산이 가능하다.

- 함수로 구현

![1527755322158](C:\Users\wips\AppData\Local\Temp\1527755322158.png)





##### 3.5.3 소프트맥스 함수의 특징#####

소프트맥스 함수를 사용하면 신경망의 출력은 다음과 같이 계산할 수 있다.

![1527755444645](C:\Users\wips\AppData\Local\Temp\1527755444645.png)

소프트맥스 함수의 총합은 1이다. 이 특징 덕분에 소프트맥스 함수의 출력을 "확률"로 해석할 수 있다.

위 예에서 y[0] = 1.8% y[1] = 24.5% y[2] = 73.7% 로 해석 가능

여기서 주의점은 *소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다는 점* 이는 지수 함수가 y = exp(x) 단조 증가 함수이기 때문이다 ???????????



**출력층의 뉴런 수 정하기**

출력층의 뉴런 수는 출려는 문제에 맞게 적절히 정해야 한다. 분류에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적이다. 