{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### 5.3 역전파\n",
    "\n",
    "\n",
    "#### 5.3.1 덧셈의 역전파\n",
    "상류에서 전해진 미분에 1을 곱하여 흘린다.\n",
    "* 1을 곱하기만 할 뿐이므로 입력된 값을 그대로 다음 노드로 보낸다.\n",
    "\n",
    "\n",
    "#### 5.3.2 곱셈의 역전파\n",
    "* 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보낸다.\n",
    " 따라서 **순방향 입력 신호의 값이 필요**하다\n",
    " \n",
    " \n",
    "### 5.4 단순한 계층 구하기\n",
    "\n",
    "신경망을 구성하는 층(신경망의 기능 단위) 각각을 하나의 클래스로 구현한다.\n",
    "예를들어 시그모이드 함수를 위한 Sigmoid, 행렬 내적을 위한 Affine 등의 기능을 계층 단위로 구현한다.\n",
    "\n",
    "#### 5.4.1 곱셈 계층\n",
    "\n",
    "* 곱셈 계층 구현"
=======
    "### 5. 오차역전파법\n",
    "\n",
    "신경망의 가중치 매개변수의 기울기(손실함수의 기울기)는 *수치 미분*을 사용해 구함\n",
    "하지만 *계산 시간이 오래 걸리는 단점*이 있음. 기울기를 더 효율적으로 구할 수 있는 **오차역전파법**\n",
    "\n",
    "\n",
    "#### 5.1 계산 그래프\n",
    "계산 그래프 : 계산 과정을 그래프로 나타낸 것. 그래프는 **노드**와 **에지**로 표현된다.\n",
    "   * 특징 \n",
    "       국소적 계산 : 전체에서 어떤 일이 일어나든 상관없이 자신과 관계된 정보만으로 다음 결과를 출력할 수 있다는 뜻. 이전 계산의 결과를 입력으로 받을 때\n",
    "               이전 계산 과정은 상관없이 단지 입력 값을 계산한다. 즉, 각 노드는 자신과 관련한 계산 (예:두 숫자의 덧셈) 외에는 아무것도 신경쓸 필요가 없다.\n",
    "   * 사용 이유 :\n",
    "       1. 국소적 계산\n",
    "       2. 중간 계산 결과를 모두 보관할 수 있다. \n",
    "       3. 미분을 효율적으로 계산할 수 있다. \n",
    "           예) 사과 가격에 대한 지불 금액의 미분 : 사과 가격의 변화에 따라 지불 금액의 변화량\n",
    "           역전파 (계산 그래프를 반대로)로 미분을 구할 수 있다.\n",
    "         \n",
    "\n",
    "### 5.2 연쇄법칙\n",
    "역전파는 국소적인 미분을 반대방향으로 전달. 전달 원리는 **연쇄법칙**에 따른다.\n",
    "\n",
    "\n",
    "#### 5.2.1 계산 그래프의 역전파\n",
    "![계산 그래프의 역전파](.\\image\\img5.PNG)\n",
    " \n",
    " 역전파 계산 절차 : 신호 E에 노드의 국소적 미분을 곱한 후 다음 노드로 전달\n",
    "     국소적 미분 : 순전파 때의 y = f(x) 계산의 미분을 구한다는 것. 이는 x에 대한 y의 미분을 구한다는 뜻.\n",
    "     E : 전달된 값\n",
    "     \n",
    "#### 5.2.2 연쇄법칙 이란??\n",
    "합성 함수 : 여러 함수로 구성된 함수. 식 5.2...2두개로 구성되다? 치환으로??\n",
    "\n",
    "연쇄법칙 : 합성 함수의 미분에 대한 성질. *합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.*\n",
    "\n",
    "\n",
    "#### 5.2.3 연쇄법칙과 계산 그래프\n",
    "예제 수식\n",
    "\n",
    "![수식 예](.\\image\\img7.PNG)\n",
    "\n",
    "위 수식의 계산 그래프\n",
    "\n",
    "![수식의 계산 그래프](.\\image\\img6.PNG)\n",
    "\n",
    "그래프의 역전파는 오른쪽에서 왼쪽으로 신호를 전파한다.\n",
    "1. 노드로 들어온 입력 신호에 \n",
    "2. 그 노드의 국소적 미분을 곱한 후 \n",
    "3. 다음 노드로 전달한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 단순한 계층 구현\n",
    "#### 5.4.1 곱셈 계층"
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 11,
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
<<<<<<< HEAD
    "    def __init__(self):    # 인스턴스 변수 x, y 초기화\n",
=======
    "    def __init__(self):\n",
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    \n",
<<<<<<< HEAD
    "    def forward(self, x, y):    #순전파\n",
=======
    "    def forward(self, x, y):\n",
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
<<<<<<< HEAD
    "    def backward(self, dout):    #역전파, 미분(dout)을 받는다.\n",
    "        dx = dout * self.y   # x와 y를 바꾼다.\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy\n",
    "    \n",
    "    "
=======
    "    def baxkward(self, dout):\n",
    "        dx = dout * self.y    # x와 y를 바꾼다\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 덧셈 계층\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout + 1\n",
    "        dy = dout + 1\n",
    "        \n",
    "        return dx, dy"
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "* 사과 쇼핑 구현"
=======
    "### 5.5 활성화 함수 계층 구현\n",
    "\n",
    "#### 5.5.1 ReLu 계층\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask - (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "    "
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 4,
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "순전파::: 220.00000000000003\n",
      "역전파::: 2.2 110.00000000000001 200\n"
=======
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "#계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(\"순전파:::\",price)\n",
    "\n",
    "#역전파\n",
    "dprice = 1\n",
    "\n",
    "# 순전파 출력에 대한 미분을 받는다.\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)    \n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(\"역전파:::\", dapple, dapple_num, dtax)"
=======
    "x = np.array([[1.0, -0.5],[-2.0, 3.0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 순전파 때의 입력 값이 0이하면 역전파때의 값은 0이 되어야 한다.\n",
    "그래서 역전파때는 순전파때 만들어둔 mask를 써서 mask의 원소가 True인 곳에는 상류에서 전파된 dout을 0으로 설정한다.\n",
    "\n",
    "#### 5.5.2 sigmoid 계층\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 순전파의 출력을 인스턴스 out 변수에 저장했다가 역전하때 그 변수를 사용해 계산한다.\n",
    "\n",
    "\n",
    "### 5.6 Affine / softmax 계층 구현\n",
    "\n",
    "#### 5.6.1 Affine 계층\n",
    "신경망의 순전파 때 수행하는 행렬의 내적은 기하학에서 Affine 변환이라고 한다. \n",
    "\n",
    "\n",
    "#### 5.6.2 배치용 Affine 계층\n",
    "데이터 N개를 묶어서 순전파 하는 경우(=배치용 Affine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        \n",
    "        return dx"
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "\n",
    "#### 5.4.2 덧셈 계층\n"
=======
    "#### 5.6.3 SoftMax-with-Loss 계층\n",
    "\n",
    "소프트맥스 함수 : 입력값을 정규화하여 출력(출력의 합이 1이 되도록 변형)\n",
    "소프트맥스 함수에 손실 함수인 **교차 엔트로피 오차도 포함**하여 softmax-with-loss 계층이라 명명\n"
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        \n",
    "        return dx, dy"
=======
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.gradient import numerical_gradient\n",
    "from common.functions import *\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None    #손실\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 벡터)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    \n",
    "    def backward(self, dout=1) :\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    " * 사과 2개(위) 귤 3개 구현"
=======
    "\n",
    "\n",
    "#### 5.7.2 오차역전파법을 적용한 신경망 구현\n",
    "\n",
    "\n"
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orange = 150\n",
    "orange_num = 3\n",
    "\n",
    "#계층들\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "\n",
    "\n",
    "#순전파\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n"
=======
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], slef.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "    # 입력 데이터 = x / 정답 레이트 = t\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "        \n",
    "        \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        if t.ndim != 1 : \n",
    "            t = np.argmax(t, axis=1)\n",
    "                \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "            \n",
    "        return accuracy\n",
    "            \n",
    "        \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "            \n",
    "        return grads\n",
    "        \n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        #순전파\n",
    "        self.loss(x, t)\n",
    "            \n",
    "        #역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "            \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "                \n",
    "        #결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "            \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 5.7.3 오차역전파법으로 구한 기울기 검증\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,50) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-40bf5d768549>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mgrad_numerical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mgrad_backProp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#각 가중치의 절대 오차의 평균을 구한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-c932c0081b7a>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m#역전파\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlastLayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-a3b3de9375d0>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,50) (3,) "
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "#데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True)\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backProp = network.gradient(x_batch, t_batch)\n",
    "\n",
    "#각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_nuimerical.keys():\n",
    "    diff = np.average(np.abs(grad_backProp[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
>>>>>>> 8bb6b92d41c60f7f42c5ec25080996e9e805babc
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
